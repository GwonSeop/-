1986년에 고안된 RNN은 시퀀스데이터 모델링에 아주 적합한 딥러닝 알고리즘으로 자리잡고 있습니다. 시간의 흐름에 따라 기록되는 타임시리즈테이터와 더불어 단어나 
어절들의 연속으로 이루어진 텍스트 역시 대표적인 시퀀스데이터로서 RNN계통 알고리즘의 주요 분석 대상입니다. 

그렇다면 RNN은 왜 시퀀스데이터 분석에 적합할까요? 그것은 RNN이 우리 두뇌의 기억 mechanism을 그대로 모방하고 있기 때문입니다.  우리 두뇌는 순서가 있는 
시퀀스데이터를 기억하는데 일종의 순차적기억력(sequential memory)를 이용하고 있습니다. 예를 들면 우리에게 알파벳을 a부터 z까지 순서대로 외워보라고 하면 누구나 쉽게 
외울 수 있지만 역순인 z부터 a순으로 기억해보라고 한다면 많은 사람들이 어려움을 겪는데 그 이유는 우리는 알파벳을 a부터 z까지의 순서를 가진 시퀀스로 학습했기 때문에
우리 두뇌는 알파벳에 대한 sequential memory(순차적 기억력)이 가지고 있다고 할 수 있습니다.

RNN은 이러한 sequential memory라는 추상적인 개념을 구현하기 위해서 neural network에 for loop을 적용합니다. for loop은 시퀀스의 앞단 정보들을 지속적으로 뒤쪽으로
pass forward하는 역할을 수행합니다. 영어로 표현하자면 "An RNN has a looping mechanism that acts as a highway to allow information to flow from one 
step to the next" 입니다. 

그런데 RNN은 시퀀스데이터를 모델링하는데 심각한 문제를 갖고 있는데 그것은 short-term memory라고 합니다. Neural network은 손실함수를 최소화하기 위해서 모든 노드의 가중치를 
갱신하는데 back propagation 알고리즘을 사용합니다. back propagation을 수행하는 과정에서 각각의 노드는 자신의 바로 전 layer에 대한 미분을 해서 자신의 가중치 변화량을 계산하는데 
끝단에서 앞단으로 갈수록 gradient값(가중치변화량)이 점점 작아져서 앞단에 위치한 은닉층에서는 학습이 거의 이루어지지 않는 vanishing gradient 문제가 발생합니다. 

Vanishing gradient문제는 RNN에서는 시퀀스데이터 앞단에 위치한 단어들이 모델 예측에 거의 반영되지 않는 short-term memory현상으로 나타나고 시퀀스데이터의 많은 정보들이 
소실되어서 모델 성능을 저하시키는 걸 알게 되었습니다.

이러한 short-term memory문제를 해결하기 위해 1997년에 LSTM(Long Short Tem Memory)이 개발되었는데 LSTM은 기본적으로는 RNN과 같이 동작하지만 
gates라고 하는 mechanism을 이용해서 short-term memory현상을 개선해나갔습니다. 